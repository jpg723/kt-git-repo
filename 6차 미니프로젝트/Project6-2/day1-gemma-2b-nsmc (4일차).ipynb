{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["8y26X2L3EecB","v0wmtPlzEs1Q","uuNKlv3pFm9V","botOy_KdFhfP","cUaVEjBUHTeB","s7B4rxr4H8H1","m0RiT4MhJc0W","NU2ryVxQKB2G","eTtY55mHSuMw","vzrC3PRcV3zL","iAgMMfZGVMJs","t6jHfNM5Vkbd","KTPB96jZZA1q","K7bzebp918si","DuWPejHu3CSV","6lVjlcRW4cAB","3FbAgg7f7sop","NnzQM71m8BD4","XzoTEHAD_Ned","M3rhBwiYAc0g","yg2Ud-xyC5yK","vI4m-RHcZIfd","vvYWXER9FqRF","TqAr-R9dFILl","6OgeDwFsFILm","-kL74CsqFILm","EtgK-6BZFILm","Cr758JhiFILm","d8SzvXk3FILm","_lmx94KCFILn","H_hpvBDpZh2-","fxUQO91_gmRA","SHBM05RHLTc8","Hu8F_oXqFILn","FNiYw1O_gOd1","Xhs39LWSgveI","sytXS8tHirfO"],"authorship_tag":"ABX9TyMykXW/5VWnrWliNmDtmBjt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Gemma-2b NSMC Finetuning"],"metadata":{"id":"i0bA2c4UEbei"}},{"cell_type":"markdown","source":["## 0. 미션\n","참조\n","- 정보: https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma?hl=ko\n","- 2b: https://huggingface.co/google/gemma-2b\n","- 2b instruction tuning: https://huggingface.co/google/gemma-1.1-2b-it\n","- 7b: https://huggingface.co/google/gemma-7b\n","- 7b instruction tuning: https://huggingface.co/google/gemma-1.1-7b-it\n","- nsmc: https://github.com/e9t/nsmc\n","\n","미션\n","* 구글에서 공개한 gemma-2b-it를 활용하여 문장의 감정을 긍정/부정으로 예측하는 감정분류 데이터셋인 nmsc를 fine-tuning을 해 보면서 LLM 동작 방법과 fine-tuning 방법을 학습한다."],"metadata":{"id":"8y26X2L3EecB"}},{"cell_type":"markdown","source":["## 1. 라이브러리 설치 (최초 한번만 실행)\n","- 라이브러리는 colab이 최초 실행 또는 종료 후 실행된 경우 한번만 실행하면 됩니다.\n","- GPU 메모리 부족등의 이유로 colab 세션을 다시 시작한 경우는 설치할 필요 없습니다.\n","- colab 세션을 다시 시작하려면 '런타임' >> '세션 다시 시작'을 선택하세요."],"metadata":{"id":"v0wmtPlzEs1Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dT1hV8-ER8a"},"outputs":[],"source":["!pip install -q -U transformers==4.38.2\n","!pip install -q -U datasets==2.18.0\n","!pip install -q -U bitsandbytes==0.42.0\n","!pip install -q -U peft==0.9.0\n","!pip install -q -U trl==0.7.11\n","!pip install -q -U accelerate==0.27.2"]},{"cell_type":"markdown","source":["## 2. 구글 드라이브 연결 (최초 한번만 실행)\n","- 구글 드라이브는 데이터 저장 및 학습 결과를 저장하기 위해서 사용합니다.\n","- 구글 드라이브는 colab이 최초 실행 또는 종료 후 실행된 경우 한번 만 연결하면 됩니다."],"metadata":{"id":"uuNKlv3pFm9V"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"oxh3YfT1GLxh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *3. 환경 (매번 필수 실행)\n","- 환경은 colab 세션을 처음 시작하거나 다시 시작한 경우 실행되어야 합니다.\n","- 프로젝트 진행에 필요한 환경을 설정합니다."],"metadata":{"id":"botOy_KdFhfP"}},{"cell_type":"markdown","source":["### 3.1. 라이브러리 Import"],"metadata":{"id":"cUaVEjBUHTeB"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import torch\n","from datasets import Dataset\n","from transformers import (AutoTokenizer,\n","                          AutoModelForCausalLM,\n","                          BitsAndBytesConfig,\n","                          pipeline,\n","                          TrainingArguments)\n","from peft import (LoraConfig,\n","                  PeftModel)\n","from trl import SFTTrainer"],"metadata":{"id":"2pfHWwihFkne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2. HuggingFace login\n","- 이번 프로젝트는 HuggingFace 로그인 해야만 진행이 가능합니다.\n","- HuggingFace 계정이 없다면 아래 URL에 접속해서 가입하시기 바랍니다.\n","  - https://huggingface.co/\n","- HuggingFace 로그인을 위해서 아래 URL에 접속해서 'User Access Token'을 생성하고 복사해서 Token에 입력하세요.\n","  - https://huggingface.co/settings/tokens"],"metadata":{"id":"s7B4rxr4H8H1"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"xAQqMz5DyMQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# access token을 복사하세요.\n","HF_TOKEN = \"\""],"metadata":{"id":"QWo7Rb-KHR9a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3. 환경정보 설정\n","- WORKSPACE\n","  - 학습 데이터 및 학습결과를 저장하기 위한 경로입니다.\n","  - 필요할 경우 적당한 경로로 변경할 수 있습니다.\n","  - 경로를 변경 할 경우 전체 경로에 공백이 포함되지 않도록 주의해 주세요.\n","- MODEL_ID\n","  - 이번 프로젝트를 위한 LLM 입니다.\n","  - 구글에서 공개한 gemma-2b를 Instruction tunned한 버전입니다.\n","  - https://huggingface.co/google/gemma-2b-it"],"metadata":{"id":"m0RiT4MhJc0W"}},{"cell_type":"code","source":["WORKSPACE = '/content/drive/MyDrive/nlp-project'\n","MODEL_ID = 'google/gemma-1.1-2b-it'"],"metadata":{"id":"_3oWzRGwJHBj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Gemma understanding (재시작 필요)\n","- Gemma의 동작 및 사용 방법을 이해하기 위한 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"NU2ryVxQKB2G"}},{"cell_type":"markdown","source":["### 4.1. model load with 4 bits\n","- 2B token을 가진 gemma를 그냥 로딩할 경우는 약 9G의 GPU vRAM이 필요합니다.\n","- 4bit 양자화를 할 경우 2.2G의 GPU vRAM 필요."],"metadata":{"id":"eTtY55mHSuMw"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          add_special_tokens=True,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"-parNbnOJ8rM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.2. pipeline\n","- https://huggingface.co/docs/transformers/main_classes/pipelines\n","- huggingface에서 inference를 쉽게 하기 위해 정의한 라이브러리."],"metadata":{"id":"vzrC3PRcV3zL"}},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=512)\n","pipe"],"metadata":{"id":"6v0OZYLUMOvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.3. gemma prompt\n","- https://huggingface.co/google/gemma-1.1-2b-it\n","- 아래와 같은 형식이 gemma의 prompt 형식 입니다.\n","```\n","<bos><start_of_turn>user\n","{content}<end_of_turn>\n","<start_of_turn>model\n","```\n","- NSMC 추론을 위한 프롬프트를 생성하는 과정입니다."],"metadata":{"id":"FOtCtWFbSsl6"}},{"cell_type":"code","source":["doc = \"\"\"엄청나게 즐거운 시간이었습니다. 강추!!!\"\"\""],"metadata":{"id":"xeUApEqfT2pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\\n\\n{}\".format(doc)\n","    }\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                            tokenize=False,\n","                                            add_generation_prompt=True)"],"metadata":{"id":"B8wHVUqYNKJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prompt)"],"metadata":{"id":"2c8yMyxdT7i7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4. gemma inference\n","- 이전 단계에서 생성한 prompt를 이용해 추론하고 결과를 확인하는 과장입니다."],"metadata":{"id":"iAgMMfZGVMJs"}},{"cell_type":"code","source":["outputs = pipe(\n","    prompt,\n","    do_sample=True,\n","    temperature=0.2,\n","    top_k=50,\n","    top_p=0.95,\n","    add_special_tokens=True\n",")\n","outputs"],"metadata":{"id":"8arCVMAdT8w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"])"],"metadata":{"id":"EXIDO8Q6Uwyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"][len(prompt):])"],"metadata":{"id":"muhxBoJzU9L-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.5. gemma chat\n","- chatbot 형식의 감정분류 예 입니다."],"metadata":{"id":"t6jHfNM5Vkbd"}},{"cell_type":"code","source":["def gen_prompt(pipe, doc):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\\n\\n{}\".format(doc)\n","        }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                                tokenize=False,\n","                                                add_generation_prompt=True)\n","    return prompt"],"metadata":{"id":"Jbyo56u8U_oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_response(pipe, doc):\n","    prompt = gen_prompt(pipe, doc)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    return outputs[0][\"generated_text\"][len(prompt):]"],"metadata":{"id":"KgpDQ7OXWNuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    doc = input('문장 > ')\n","    doc = doc.strip()\n","    if len(doc) == 0:\n","        break\n","    result = gen_response(pipe, doc)\n","    print(f'감정 > {result}\\n\\n')"],"metadata":{"id":"Ey87FSqeWuur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Gemma-2b 학습 tutorial (재시작 필요)\n","- Gemma의 학습과정을 이해하기 위한 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"KTPB96jZZA1q"}},{"cell_type":"markdown","source":["### 5.1. Gemma를 로딩 (4bit)"],"metadata":{"id":"K7bzebp918si"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",  # NormalFloat 4\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          add_special_tokens=True,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"vusLDZVEX2br"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2. LoRA를 적용하기 위한 설정을 정의"],"metadata":{"id":"DuWPejHu3CSV"}},{"cell_type":"code","source":["# lora config\n","lora_config = LoraConfig(\n","    r=6,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"xQRoeFMq28FC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.3. Dataset 정의\n","- 동작 확인을 위해서 최소한의 데이터만을 사용합니다."],"metadata":{"id":"6lVjlcRW4cAB"}},{"cell_type":"code","source":["# huggingface dataset을 생성하는 함수입니다.\n","def make_dataset(df, sample=-1):\n","    df = df[['document', 'label']]\n","    if sample > 0:\n","        df = df.sample(sample)\n","    dataset = Dataset.from_pandas(df)\n","    return dataset"],"metadata":{"id":"mg93IM9p7Msr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = {\n","    'document': [\n","        '영화 강추 합니다.',\n","        '시간이 너무 아깝습니다.'\n","    ],\n","    'label': [\n","        1,\n","        0\n","    ]\n","}\n","df_train = pd.DataFrame(data)\n","df_train"],"metadata":{"id":"BypeOlNx7b1O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = make_dataset(df_train)\n","train_dataset"],"metadata":{"id":"sRNMFpNo7ez-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.4. 학습을 위한 prompt 생성 함수 정의"],"metadata":{"id":"3FbAgg7f7sop"}},{"cell_type":"code","source":["# 학습을 위한 prompt를 생성합니다.\n","def gen_train_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        doc = example['document'][i]\n","        label = '긍정' if example['label'][i] == 1 else '부정'\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\n","\n","{}<end_of_turn>\n","<start_of_turn>model\n","{}<end_of_turn><eos>\"\"\".format(doc, label))\n","    return prompt_list"],"metadata":{"id":"TWBjzn937yxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt 동작을 확인합니다.\n","prompt = gen_train_prompt(train_dataset[:1])[0]\n","print(prompt)"],"metadata":{"id":"H9rDKzt78D6Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.5. Trainer를 이용한 학습\n","- https://huggingface.co/docs/trl/sft_trainer\n","- https://huggingface.co/docs/trl/v0.8.4/en/sft_trainer#trl.SFTTrainer\n","- https://huggingface.co/docs/transformers/v4.39.3/en/main_classes/trainer#transformers.TrainingArguments"],"metadata":{"id":"NnzQM71m8BD4"}},{"cell_type":"code","source":["# trainer 정의\n","trainer = SFTTrainer(\n","    model=model, # 학습할 모델\n","    train_dataset=train_dataset,  # 학습할 데이터 셋\n","    max_seq_length=256,  # 최대 토큰 갯수\n","    args=TrainingArguments(\n","        output_dir=os.path.join(WORKSPACE, \"nsmc-tutorial\"),\n","        # num_train_epochs = 1,  # epoc으로 할 경우 너무 많이 걸리 수 있음\n","        max_steps=10,  # 학습 step 수\n","        per_device_train_batch_size=2,  # gpu당 입력 batch_size\n","        gradient_accumulation_steps=4,  # gradient 누적 후 학습\n","        optim=\"paged_adamw_8bit\",  # optimizer (QLoRA)\n","        warmup_steps=1000,  # learning rate warmup step\n","        learning_rate=1e-4,  # learning rate\n","        # bf16=True,  # bf16 사용 여부 (3090 이상에서 가능)\n","        fp16=True,  # fp16 사용 여부 (예전 GPU에서 사용 가능, T4)\n","        logging_steps=10,  # 얼마만에 한번 씩 중간 결과를 확인할 것인가?\n","        push_to_hub=False,  # huggingface에 올릴 수 있음\n","        report_to=None,  # W&B에 학습결과 공유 가능\n","    ),\n","    peft_config=lora_config,  # QLoRA config\n","    formatting_func=gen_train_prompt,  # 프롬프트 생성 함수\n",")"],"metadata":{"id":"6fjy0HQe8Arm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train\n","trainer.train()"],"metadata":{"id":"DLlY-PMg_En2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.6. LoRA의 학습된 weight 저장\n","- 전체 weight가 아닌 변화량만 저장하므로 매우 작은 용량을 차지합니다."],"metadata":{"id":"XzoTEHAD_Ned"}},{"cell_type":"code","source":["type(trainer.model)"],"metadata":{"id":"RZ8mh9vN74yh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save lora (delta weight)\n","trainer.model.save_pretrained(\"lora_adapter\")"],"metadata":{"id":"jSeDP-6V_MDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장된 결과 확인\n","!ls -lh ./lora_adapter"],"metadata":{"id":"gNiJxCdr_-N4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.7. Original 모델에 LoRA를 학습된 결과를 더해서 최종 모델 저장"],"metadata":{"id":"M3rhBwiYAc0g"}},{"cell_type":"code","source":["# original model load (before finetuned)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             torch_dtype=torch.float16,\n","                                             token=HF_TOKEN)"],"metadata":{"id":"jwiqy7DAAcDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge : original + delta wieght\n","model = PeftModel.from_pretrained(model,\n","                                  \"lora_adapter\",\n","                                  device_map='auto',\n","                                  torch_dtype=torch.float16,\n","                                  token=HF_TOKEN)\n","model = model.merge_and_unload()"],"metadata":{"id":"M5F6ErFmA4Tt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save fine-tunned model\n","model.save_pretrained(os.path.join(\"nsmc-tutorial\", \"checkpoint-final\"))"],"metadata":{"id":"szwzA6LQA8Z2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장 결과 확인\n","!ls -lh nsmc-tutorial/checkpoint-final"],"metadata":{"id":"MTYUFew4CoRD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.8. 학습 결과 평가"],"metadata":{"id":"yg2Ud-xyC5yK"}},{"cell_type":"code","source":["# 평가 데이터\n","data = {\n","    'document': [\n","        '영화 재밌어요',\n","        '졸려서 눈물이 났어요.'\n","    ],\n","    'label': [\n","        1,\n","        0\n","    ]\n","}\n","df_test = pd.DataFrame(data)\n","df_test"],"metadata":{"id":"cVFUJVcODBMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가 데이터셋 생성\n","test_dataset = make_dataset(df_test)\n","test_dataset"],"metadata":{"id":"PvXYWvz9DA_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가를 위한 prompt\n","def gen_test_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        doc = example['document'][i]\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\n","\n","{}<end_of_turn>\n","<start_of_turn>model\n","\"\"\".format(doc))\n","    return prompt_list"],"metadata":{"id":"pTP43bRUDmTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt 확인\n","prompt = gen_test_prompt(test_dataset[1:])[0]\n","print(prompt)"],"metadata":{"id":"pRQed01jDua8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pipeline 정의\n","pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=10)"],"metadata":{"id":"MCjH5uuIENCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# infer\n","total_sample_cnt, total_correct_cnt = 0, 0\n","for example in test_dataset.iter(1):\n","    label = '긍정' if example['label'][0] == 1 else '부정'\n","\n","    prompt = gen_test_prompt(example)\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    pred = outputs[0][0]['generated_text'][len(prompt[0]):]\n","    total_sample_cnt += 1\n","    total_correct_cnt += 1 if label == pred else 0\n","\n","    print(example['document'][0], \":\", pred)\n","    print('-' * 20)\n","print(f\"Test Accuracy: {total_correct_cnt} / {total_sample_cnt} = {total_correct_cnt/total_sample_cnt:.4f}\")"],"metadata":{"id":"TP3ILJIbD51P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Gemma-2b 학습 실습 (재시작 필요)\n","- '5. Gemma-2b 학습 tutorial'를 참고해서 실제로 Gemma-2b를 학습해 보는 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"vI4m-RHcZIfd"}},{"cell_type":"markdown","source":["### 6.1 학습 및 테스트 데이터를 다운로드\n","- https://github.com/e9t/nsmc"],"metadata":{"id":"vvYWXER9FqRF"}},{"cell_type":"code","source":["# 데이터를 다운로드할 폴더를 생성합니다.\n","os.makedirs(os.path.join(WORKSPACE, \"data\", \"nsmc\"), exist_ok=True)"],"metadata":{"id":"ORdpV5avFuZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://github.com/e9t/nsmc/raw/master/ratings_train.txt \\\n","    -O {WORKSPACE}/data/nsmc/train.tsv"],"metadata":{"id":"A5vOLM0sGeuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://github.com/e9t/nsmc/raw/master/ratings_test.txt \\\n","    -O {WORKSPACE}/data/nsmc/test.tsv"],"metadata":{"id":"bqc8HZLZHGUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls {WORKSPACE}/data/nsmc"],"metadata":{"id":"_ET3-W0kHLBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(os.path.join(WORKSPACE, \"data\", \"nsmc\", \"train.tsv\"),\n","                       sep='\\t')\n","df_train"],"metadata":{"id":"vKCbr8suHOci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = pd.read_csv(os.path.join(WORKSPACE, \"data\", \"nsmc\", \"test.tsv\"),\n","                      sep='\\t')\n","df_test"],"metadata":{"id":"3Er9-LxlHj2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.2. Gemma를 로딩 (4bit)"],"metadata":{"id":"TqAr-R9dFILl"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          add_special_tokens=True,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"HyO_QaEsFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.3. LoRA를 적용하기 위한 설정 정의"],"metadata":{"id":"6OgeDwFsFILm"}},{"cell_type":"code","source":["# lora config\n","lora_config = LoraConfig(\n","    r=6,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"7Up-NnBGFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.4. Train Dataset을 생성\n","- 이전에 다운 받은 {WORKSPACE}/data/nsmc/train.tsv 파일을 학습 데이터로 사용합니다.\n","- 학습 데이터는 df_train으로 이미 읽어진 상태입니다."],"metadata":{"id":"-kL74CsqFILm"}},{"cell_type":"code","source":["# huggingface dataset을 생성하는 함수입니다.\n","def make_dataset(df, sample=-1):\n","    df = df[['document', 'label']]\n","    if sample > 0:\n","        df = df.sample(sample)\n","    dataset = Dataset.from_pandas(df)\n","    return dataset"],"metadata":{"id":"rJm0Pu8XFILm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 15만개의 학습 데이터셋을 생성합니다.\n","train_dataset = make_dataset(df_train)\n","train_dataset"],"metadata":{"id":"Y0bOx7NMFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.5. 학습을 위한 prompt 생성 함수를 정의합니다."],"metadata":{"id":"EtgK-6BZFILm"}},{"cell_type":"code","source":["# 학습을 위한 prompt를 생성합니다.\n","def gen_train_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        doc = example['document'][i]\n","        label = '긍정' if example['label'][i] == 1 else '부정'\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\n","\n","{}<end_of_turn>\n","<start_of_turn>model\n","{}<end_of_turn><eos>\"\"\".format(doc, label))\n","    return prompt_list"],"metadata":{"id":"ijF0wYW_FILm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt 동작을 확인합니다.\n","prompt = gen_train_prompt(train_dataset[:1])[0]\n","print(prompt)"],"metadata":{"id":"abd-Ok8rFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.6. Trainer를 이용한 학습\n","- https://huggingface.co/docs/trl/sft_trainer\n","- https://huggingface.co/docs/trl/v0.8.4/en/sft_trainer#trl.SFTTrainer\n","- https://huggingface.co/docs/transformers/v4.39.3/en/main_classes/trainer#transformers.TrainingArguments\n","- tutorial의 설정 중 다음 내용만 변경하셔도 됩니다.\n","  - output_dir: {WORKSPACE}/nsmc-gemma\n","  - max_steps: 1000 ~ 2000 사이\n","  - logging_steps: 100"],"metadata":{"id":"Cr758JhiFILm"}},{"cell_type":"code","source":["# trainer 정의\n","trainer = SFTTrainer(\n","    model=model, # 학습할 모델\n","    train_dataset=train_dataset,  # 학습할 데이터 셋\n","    max_seq_length=256,  # 최대 토큰 갯수\n","    args=TrainingArguments(\n","        output_dir=os.path.join(WORKSPACE, \"nsmc-gemma\"),\n","        # num_train_epochs = 1,  # epoc으로 할 경우 너무 많이 걸리 수 있음\n","        max_steps=1000,  # 학습 step 수 (1000 ~ 2000 사이)\n","        per_device_train_batch_size=2,  # gpu당 입력 batch_size\n","        gradient_accumulation_steps=4,  # gradient 누적 후 학습\n","        optim=\"paged_adamw_8bit\",  # optimizer (QLoRA)\n","        warmup_steps=1000,  # learning rate warmup step\n","        learning_rate=1e-4,  # learning rate\n","        # bf16=True,  # bf16 사용 여부 (3090 이상에서 가능)\n","        fp16=True,  # fp16 사용 여부 (예전 GPU에서 사용 가능, T4)\n","        logging_steps=100,  # 얼마만에 한번 씩 중간 결과를 확인할 것인가?\n","        push_to_hub=False,  # hugingface에 올릴 수 있음\n","        report_to=None,  # W&B에 학습결과 공유 가능\n","    ),\n","    peft_config=lora_config,  # QLoRA config\n","    formatting_func=gen_train_prompt,  # 프롬프트 생성 함수\n",")"],"metadata":{"id":"F0J_oLa3FILm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train\n","trainer.train()"],"metadata":{"id":"L4FZg34sFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.7. LoRA의 학습된 weight 저장\n","- 전체 weight가 아닌 변화량만 저장하므로 매우 작은 용량을 차지합니다."],"metadata":{"id":"d8SzvXk3FILm"}},{"cell_type":"code","source":["# save lora (delta weight)\n","trainer.model.save_pretrained(\"lora_adapter\")"],"metadata":{"id":"_R3uZTZJFILm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장된 결과 확인\n","!ls -lh ./lora_adapter"],"metadata":{"id":"ivGbXnWvFILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.8. Original 모델에 LoRA를 학습된 결과를 더해서 최종 모델 저장.\n","- 최종 모델 저장 위치: {WORKSPACE}/nsmc-gemma/checkpoint-final"],"metadata":{"id":"_lmx94KCFILn"}},{"cell_type":"code","source":["# original model load (before finetuned)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             torch_dtype=torch.float16,\n","                                             token=HF_TOKEN)"],"metadata":{"id":"e6HmVavKFILn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge : original + delta wieght\n","model = PeftModel.from_pretrained(model,\n","                                  \"lora_adapter\",\n","                                  device_map='auto',\n","                                  torch_dtype=torch.float16,\n","                                  token=HF_TOKEN)\n","model = model.merge_and_unload()"],"metadata":{"id":"8zcbtfukFILn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save fine-tunned model\n","model.save_pretrained(os.path.join(WORKSPACE, \"nsmc-gemma\", \"checkpoint-final\"))"],"metadata":{"id":"r062EPr9FILn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장 결과 확인\n","!ls -lh {WORKSPACE}/nsmc-gemma/checkpoint-final"],"metadata":{"id":"dHJ228XXFILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Gemma-2b 평가 및 추론 실습 (재시작 필요)\n","- '6. Gemma-2b 학습 실습'을 완료한 후 결과를 평가해 보는 과정입니다.\n","- '5. Gemma-2b 학습 tutorial'의 '5.8. 학습결과 평가' 섹션을 참고하세요.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"H_hpvBDpZh2-"}},{"cell_type":"markdown","source":["### 7.1. Test Data loading\n","- 평가를 위한 테스트 데이터를 로딩합니다."],"metadata":{"id":"fxUQO91_gmRA"}},{"cell_type":"code","source":["df_test = pd.read_csv(os.path.join(WORKSPACE, \"data\", \"nsmc\", \"test.tsv\"),\n","                      sep='\\t')\n","df_test"],"metadata":{"id":"leRLYA4HgeG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.2. 학습된 Gemma를 로딩 (4bit)\n","- AutoModelForCausalLM의 MODEL_ID를 '{WORKSPACE}/nsmc-gemma/checkpoint-final'로 변경합니다.\n","- 위 경로가 우리가 학습한 모델이 저장된 위치입니다.\n","- AutoTokenizer는 MODEL_ID를 그대로 사용합니다."],"metadata":{"id":"SHBM05RHLTc8"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model_fn = os.path.join(WORKSPACE, \"nsmc-gemma\", \"checkpoint-final\")\n","model = AutoModelForCausalLM.from_pretrained(model_fn,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          add_special_tokens=True)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"isN5svyFLTc8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.3. 학습 결과 평가를 위한 데이터 준비"],"metadata":{"id":"Hu8F_oXqFILn"}},{"cell_type":"code","source":["# huggingface dataset을 생성하는 함수입니다.\n","def make_dataset(df, sample=-1):\n","    df = df[['document', 'label']]\n","    if sample > 0:\n","        df = df.sample(sample)\n","    dataset = Dataset.from_pandas(df)\n","    return dataset"],"metadata":{"id":"cC4rptU8gVek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가 데이터셋 새성\n","test_dataset = make_dataset(df_test)\n","test_dataset"],"metadata":{"id":"s2MidKsYFILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.4. 테스트를 위한 프롬프트 생성 및 확인"],"metadata":{"id":"FNiYw1O_gOd1"}},{"cell_type":"code","source":["# 평가를 위한 prompt\n","def gen_test_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        doc = example['document'][i]\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\n","\n","{}<end_of_turn>\n","<start_of_turn>model\n","\"\"\".format(doc))\n","    return prompt_list"],"metadata":{"id":"rfsWxXKZFILn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt 확인\n","prompt = gen_test_prompt(test_dataset[1:])[0]\n","print(prompt)"],"metadata":{"id":"3D_IBuK3FILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.5. 파이프라인 정의 및 평가\n","- 평가는 1000개만 실행하세요. (시간이 오래 걸립니다.)"],"metadata":{"id":"Xhs39LWSgveI"}},{"cell_type":"code","source":["# pipeline 정의\n","pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=10)"],"metadata":{"id":"Sy6h6fygFILn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# infer\n","total_sample_cnt, total_correct_cnt = 0, 0\n","for example in tqdm(test_dataset.iter(1)):\n","    label = '긍정' if example['label'][0] == 1 else '부정'\n","\n","    prompt = gen_test_prompt(example)\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    pred = outputs[0][0]['generated_text'][len(prompt[0]):]\n","    total_sample_cnt += 1\n","    total_correct_cnt += 1 if label == pred else 0\n","\n","    # print(example['document'][0], \":\", pred)\n","    # print('-' * 20)\n","\n","    if total_sample_cnt >= 1000:\n","        break\n","print(f\"Test Accuracy: {total_correct_cnt} / {total_sample_cnt} = {total_correct_cnt/total_sample_cnt:.4f}\")"],"metadata":{"id":"vkaERK2SFILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.6. gemma chat\n","- chatbot 형식의 감정분류 입니다.\n","- '4.5. gemma chat' 섹션을 참고하세요."],"metadata":{"id":"sytXS8tHirfO"}},{"cell_type":"code","source":["def gen_prompt(pipe, doc):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\\n\\n{}\".format(doc)\n","        }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                                tokenize=False,\n","                                                add_generation_prompt=True)\n","    return prompt"],"metadata":{"id":"zyA1Li8QZlCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_response(pipe, doc):\n","    prompt = gen_prompt(pipe, doc)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    return outputs[0][\"generated_text\"][len(prompt):]"],"metadata":{"id":"RAqKA2fHjGp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    doc = input('문장 > ')\n","    doc = doc.strip()\n","    if len(doc) == 0:\n","        break\n","    result = gen_response(pipe, doc)\n","    print(f'감정 > {result}\\n\\n')"],"metadata":{"id":"yUJJYA_njJY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xl6PLOhZjMUT"},"execution_count":null,"outputs":[]}]}